% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/copss.R
\name{copss_oppelt}
\alias{copss_oppelt}
\title{COPSS method with OP or PELT algorithm}
\usage{
copss_oppelt(
  X,
  W,
  cp_alg = cp_hdmean_op,
  penaltyform = "CV",
  normalized = TRUE
)
}
\arguments{
\item{X}{N*p matrix}

\item{W}{if normalized is TRUE, then W is inverse of estimated_sigma_square(X); 
if is FALSE, then W = rep(1, ncol(X)).}

\item{cp_alg}{Change-point detection algorithm}

\item{penaltyform}{The penalty form, can be "BIC", "WANG", "ZOU" ,"CV", ""}

\item{normalized}{Bool, logical, TRUE or FALSE}
}
\value{
A list of two matrices mat_e and mat_o. 
  mat_o: which uses 'odd' data to estimate, CV error matrix on 'even' data.
  mat_e: which uses 'even' data to estimate, CV error matrix on 'odd' data.
}
\description{
A cross-validation method to find the optimal number of change-points 
with change detection algorithm being OP or PELT.
}
\section{Details}{

The 'normalized' parameter indicates whether the raw data X is normalized 
or not, which can be used in the cost function in DP, OP, PELT algorithm 
as well as the cv objective function cv_objfun(Xo, Xo, W, cp_o).

For usage in cost function:
  W = estimated_sigma_square(X); # p-len vector.
  Y(i, j) = X(i, j) / pow(W[j], 0.5);
  ||W^(-1/2)(X-X_bar)||^2;
  cost_matrix(X, normalized = true);
  which is embeded in the cost_matrix function, and that is where the 
  normalized parameter is used.
 
NOTES:
  Actually, in DP, it doesn't matter whether the raw data X is normalized 
  or not, because there is no penalty in the objective function.
  
For usage in cv objective function:
  it becomes whether we use the identity matrix as covariance matrix.
  TRUE, Wn = I_{p*p} as the covariance matrix;
  FALSE, Wn = estimated_sigma_square(X) as the covariance matrix;
  
NOTES:
  Namely, for X data, when calculating cost_matrix, normalized nor not.
\itemize{
  \item{For n=p=200, if normalized = FALSE, the algorithm does not work.}
  \item{For n=200, p=4, whatever normalized is, the algorithm works well.}
 }

cp_alg can take values: cp_alg = cp_hdmean_op, cp_alg = cp_hdmean_pelt.

Function copss_oppelt checks all the possible combinations of alph and c0,
  for given X matrix, i.e., grid searching for (alph, c0). 
  Finally, we need to output the optimal K_hat, which leads to the minimum 
  C(M_{L}^{odd}, Xeven). 

We store the [alph, c0, pen, obj_o, K_hat_o, tau_hat_o].

The penalty of the OP or PELT can take pen = p + c0 * log(n)^(1 + alph).

SOME TRICKS:
\itemize{
  \item{1. The bigger of the penalty the less number of the change-points.}
  \item{2. Different (alph, c0), i.e. different penalty can lead to same K_hat.}
  \item{3. If the K_hat is the same, then the tau_hat must be the same.}
  \item{4. The result of K_hat can be [0, 1, 3, 5, 12, 20], not every integer.}
  \item{5. We can store the results in a matrix, or two matrices}
 }

(alph, c0, pen, obj_o, obj_oe, K_hat_o, tau_hat_o) OR 
(alph, c0, pen, obj_e, obj_eo, K_hat_e, tau_hat_e), (e.g. 6 + K_max)

Of course, they are not of the same length, so we need an extra variable 
  to keep record of the max K_hat, say, Ke_max.

For the CV penalty: pen = p + c0 * (log(n))^(1 + alph),
for example: alph = 0.1; c0 = 1.5.
}

