
# PS: TO DEAL with the numerical small values!
# if (obj_o < 1e-4) {obj_o <- 0;}  # IMPORTANT!!!


#### SOME TRICKS: 
# 1. The bigger of the penalty the less number of the change-points.
# 2. Different (alph, c0), which means different penalty can lead to same K_hat;
# 3. If the K_hat is the same, then the tau_hat must be the same.
# 4. The result of K_hat can be [0, 1, 3, 5, 12, 20], not taking every integer.
## For OP/PELT, ideally, when K_hat is the same, the tau_hat must be the same,
## However, sometimes, even K_hat is the same, the estimated locations of the 
## change-points are not exactly the same, example, (..27..) and (..25..).
## So we need a flag=TRUE, if there are duplicated K_hat but with slightly 
## different locations of mat_e or mat_o. 
## When duplicated K_hat happens, GIVES a FLAG = TRUE, we sort the mat_o/mat_e 
## according to the penalty, then keep the row with the bigger penalty.

#### TRICKES: NEED a FLAG = TRUE

#' High-dimensinal CV objective function - Cross-validation value
#' 
#' The high dimensional mean change model, the CV function.
#' 
#' NOTE: Never use this function in hBIC_dp_hausdorff, bacause we need 
#'   the cv_objfun(X, X, W, cp_hat) in hBIC to calculate the fitting error
#'   and then plus the penalty, which is calculated by 
#'   calculate_penalty(X, normalized = TRUE, penaltyform = "hBIC", alph, c0)
#'   The p has already been included in penalty term, if we use this function
#'   again, we'll add another p.
#' 
#' @param Xo dataset used to estimate, matrix of n1*p
#' @param Xe dataset used to CV, matrix of n2*p
#' @param W weight matrix, p*p
#' @param cp_hat The estimated change-point, with 0 and 1 included
#' @return cv_objfun(Xo, Xe, W, cp_hat) - L * p
hdmean_objfun_cv <- function(Xo, Xe, W, cp_hat){
  p <- dim(Xo)[2]
  L <- length(cp_hat) - 2  # c(0, tau_hat, 1)
  v <- cv_objfun(Xo, Xe, W, cp_hat) - L * p
  # v <- cv_objfun(Xo, Xe, W, cp_hat)
  return(v)
}


#' Calculate the trace of \eqn{R^2} using R
#' 
#' Calculate the trace of \eqn{R^2}, \eqn{R^2} is correlation matrix of data X
#' 
#' If we use the penalty from Yunlong Wang paper, which is calculated
#' \deqn{ penalty = p + c_0 * (trace(R^2))^{1/2} * log(n)^{1+\alpha}}
#' Where
#' \deqn{
#' trace(R^2) = 1 /(4(n-3)) \sum_{i=1}^{n-3}
#'   Z_{i}^T * (D_{-(i,i+1,i+2,i+3)})^(-1) * Z_{i+2}
#' }
#' with \eqn{ Z_{i} = X_{[i,]} - X_{[i+1,]} } }.
#' 
#' @param X n*p data matrix
#' @return the value of trace(\eqn{R^2})
#' 
#' @examples
#' X <- matrix(1:100, 20, 5)
#' trace_R_square(X)
#' 
#' # X <- matrix(rnorm(2048 * 6000), 2048, 6000)
#' # system.time( tr2 <- trace_R_square_cpp(X) )
#' # user   system  elapsed 
#' # 977.470   63.005 1041.387
#' 
#' # system.time( tr2 <- trace_R_square(X) )
#' # user  system elapsed 
#' # 350.799 220.544 571.491 
trace_R_square <- function(X){
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  s <- 0
  for (i in 1:(n-3)) {  # i = 1
    X1 <- X[-(i:(i+3)), ]
    D <- estimated_sigma_square(X1)
    e <- t(X[i,] - X[i+1,]) %*% diag(D^(-1)) %*% (X[i+2,] - X[i+3,])
    s <- s + e^2
  }
  res <- s/(4*(n-3))
  return(res[1, 1])
}


#' Calculate penalty value
#' 
#' Calculate penalty for the OP and PELT change detection algorithm.
#' 
#' This function calls \code{\link{trace_R_square}}, 
#' and it can only be called before normalization. 
#' The result of this function can be used in the OP and PELT function.
#' 
#' The penalty:
#' \deqn{penalty = c_0 * \log(n)^{1 + \alpha}}.
#' 
#' The data: 
#' \deqn{X_i = \mu_k + \sigma \times e_i . }
#' If we set  \eqn{\sigma = 0}, usually we use this kind of data to check if 
#' the codes of OP and PELT algorithms work fine.
#' 
#' The data can be generated by \code{\link{generate_1D_data}}
#'   \deqn{x_{i} = \mu_{j} + sig_constant * \epsilon_{i}. }
#' If we set \code{sig_constant = 0}, usually we use this kind of data to check 
#' if the codes of OP and PELT work fine.
#'   
#' NOTES1: In high dimensions, we ALWAYS need LARGE penalty. 
#' 
#' Notice the fact that the parameter normalized only works with 
#'   penaltyform = "WANG", "hBIC" and "CV". Because these three use traceR2.
#'   If p is large, e.g. p=400, n=200, 
#'   we CAN use "WANG", "hBIC", "CV" in high dimension settings.
#'   
#' NOTES2: In low dimensions, we ALWAYS need SMALL penalty.
#'   If p is small, we can set penaltyform ANY value, "BIC", "ZOU", "WANG", "CV".
#'   
#' NOTES3: If p is large, BUT we need SMALL penalty (in case someone want this 
#'   kind of setting, in practice we usually do not need this setting).
#'   So we can ONLY set penaltyform = "BIC", "ZOU", "", 
#'   however we cannot set "WANG", "hBIC" ,"CV".
#' 
#' EG1: when n=200, p=4, low dimension, we need SMALL values of penalty,
#'   such as 9 or 20, for both case of sig_constant=0, and 1. so we CAN set 
#'   penaltyform = any string includes "BIC", "ZOU", "WANG", "hBIC", "CV", "".
#'   
#' EG2: when n=200, p=400, high dimension, we need LARGE values of penalty, 
#'   such as 200, or 300, for case of sig_constant=1; 
#'   while for the case of sig_constant=0, we need SMALL penalty by setting 
#'   penaltyform = "BIC", "ZOU", "".
#' 
#' HOWEVER, when sig_constant=0, which is usually used for algorithm checking, 
#'   even though we have high dimension with p=400, we need SMALL penalty. 
#'   
#' In one word: 
#' \itemize{
#' \item{p=4, small penalty only, for both sig_constant=0 and 1; penaltyform 
#'   can be any string includes "BIC", "ZOU", "WANG", "hBIC" ,"CV", "";
#'   Both normalized=TRUE or FALSE will be fine, no difference.}
#' \item{p=400, large penalty for sig_constant=1, and normalized=TRUE,
#'   penaltyform = "WANG", "hBIC", "CV";
#'   If we take normalized=FALSE, no change-points will be detected.}
#' \item{p=400, small penalty for sig_constant=0, and normalized=FALSE,
#'   penaltyform = "BIC", "ZOU", "";
#'   If we take normalized=TRUE, no change-points will be detected.}
#'  }
#' 
#' @param X NumericMatrix
#' @param normalized bool, if we use the identity matrix \eqn{I_{p}} as the 
#'   standardization. TRUE means we calculate another matrix and we do not have
#'   to use the identity matrix as the scaler standardization.
#' 
#' @param penaltyform string, specifies the penalty form of the objective function.
#'   "BIC", "ZOU", "", gives SMALL penalty;
#'   "WANG", "hBIC", "CV", gives LARGE penalty.
#' @param alph double, pen = c0 * log(n)^(1 + alph).
#' @param  c0 double
#' @return The penalty value. 
#'   The result of this function can be used in the OP and PELT functions.
calculate_penalty <- function(
  X, normalized = TRUE, penaltyform = "BIC", alph = 0.0, c0 = 1.0) {
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  if (normalized) {traceR2 = trace_R_square(X) } else {traceR2 = p}
  
  if (penaltyform == "BIC") {
    pen = c0 * log(n)^(1.0 + alph)
  } else if (penaltyform == "ZOU"){
    pen = c0 * log(n)^(2.1) / 2
  } else if (penaltyform == "WANG") {
    # the penalty from Yunlong Wang's paper, with fixing 1.1 and 2.5.
    pen = p + 2.5 * traceR2^(0.5) * log(n)^(1.1)
  } else if (penaltyform == "hBIC") {
    pen = p + c0 * traceR2^(0.5) * log(n)^(1.0 + alph)
  } else if (penaltyform == "CV") {
    pen = p + c0 * traceR2^(0.5) * log(n)^(1.0 + alph)
  } else {
    pen = log(n)^(1.1)
  }
  return(pen)
}



#' COPSS method with OP or PELT algorithm 
#' 
#' A cross-validation method to find the optimal number of change-points 
#' with change detection algorithm being OP or PELT.  
#' 
#' @section Details:
#' The 'normalized' parameter indicates whether the raw data X is normalized 
#' or not, which can be used in the cost function in DP, OP, PELT algorithm 
#' as well as the cv objective function cv_objfun(Xo, Xo, W, cp_o).
#' 
#' For usage in cost function:
#'   W = estimated_sigma_square(X); # p-len vector.
#'   Y(i, j) = X(i, j) / pow(W[j], 0.5);
#'   ||W^(-1/2)(X-X_bar)||^2;
#'   cost_matrix(X, normalized = true);
#'   which is embeded in the cost_matrix function, and that is where the 
#'   normalized parameter is used.
#'  
#' NOTES:
#'   Actually, in DP, it doesn't matter whether the raw data X is normalized 
#'   or not, because there is no penalty in the objective function.
#'   
#' For usage in cv objective function:
#'   it becomes whether we use the identity matrix as covariance matrix.
#'   TRUE, Wn = I_{p*p} as the covariance matrix;
#'   FALSE, Wn = estimated_sigma_square(X) as the covariance matrix;
#'   
#' NOTES:
#'   Namely, for X data, when calculating cost_matrix, normalized nor not.
#' \itemize{
#'   \item{For n=p=200, if normalized = FALSE, the algorithm does not work.}
#'   \item{For n=200, p=4, whatever normalized is, the algorithm works well.}
#'  }
#' 
#' cp_alg can take values: cp_alg = cp_hdmean_op, cp_alg = cp_hdmean_pelt.
#' 
#' Function copss_oppelt checks all the possible combinations of alph and c0,
#'   for given X matrix, i.e., grid searching for (alph, c0). 
#'   Finally, we need to output the optimal K_hat, which leads to the minimum 
#'   C(M_{L}^{odd}, Xeven). 
#' 
#' We store the [alph, c0, pen, obj_o, K_hat_o, tau_hat_o].
#' 
#' The penalty of the OP or PELT can take pen = p + c0 * log(n)^(1 + alph).
#' 
#' SOME TRICKS:
#' \itemize{
#'   \item{1. The bigger of the penalty the less number of the change-points.}
#'   \item{2. Different (alph, c0), i.e. different penalty can lead to same K_hat.}
#'   \item{3. If the K_hat is the same, then the tau_hat must be the same.}
#'   \item{4. The result of K_hat can be [0, 1, 3, 5, 12, 20], not every integer.}
#'   \item{5. We can store the results in a matrix, or two matrices}
#'  }
#' 
#' (alph, c0, pen, obj_o, obj_oe, K_hat_o, tau_hat_o) OR 
#' (alph, c0, pen, obj_e, obj_eo, K_hat_e, tau_hat_e), (e.g. 6 + K_max)
#'
#' Of course, they are not of the same length, so we need an extra variable 
#'   to keep record of the max K_hat, say, Ke_max.
#' 
#' For the CV penalty: pen = p + c0 * (log(n))^(1 + alph),
#' for example: alph = 0.1; c0 = 1.5. 
#' 
#' @inheritParams copss_dp
#' @param X N*p matrix
#' @param cp_alg Change-point detection algorithm
#' @param penaltyform The penalty form, can be "BIC", "WANG", "ZOU" ,"CV", ""
#' @param normalized Bool, logical, TRUE or FALSE
#' @return A list of two matrices mat_e and mat_o. 
#'   mat_o: which uses 'odd' data to estimate, CV error matrix on 'even' data.
#'   mat_e: which uses 'even' data to estimate, CV error matrix on 'odd' data.
copss_oppelt <- function(
    X, W, cp_alg = cp_hdmean_op, penaltyform = "CV", normalized = TRUE) {
  ## cp_alg = cp_hdmean_op; penaltyform = "CV"; normalized = TRUE
  ## Xo, Xe are Xodd, Xeven repectively;
  ## err_e matrix, where Xe is used to estimate, Xo is to test;
  ## err_o matrix, where Xo is used to estimate, Xe is to test.
  Xo <- X[seq(1, nrow(X), 2), ]  # max(seq(1, nrow(X), 2))
  Xe <- X[seq(2, nrow(X), 2), ]  # max(seq(2, nrow(X), 2))
  if (!is.matrix(Xo)) {Xo = matrix(Xo, length(Xo), 1)}
  if (!is.matrix(Xe)) {Xe = matrix(Xe, length(Xe), 1)}
  
  ##  GIVEN a GOOD sequence of penalty values
  alph_list <- c(0, 0.1, 0.3, 0.4, 0.5, 1, 1.1)
  c0_list <- seq(0.1, 10, by=0.1)
  eg <- expand.grid(alph_list, c0_list)
  n_param <- nrow(eg)
  h_names <- c("alpha", "c0", "pen", "objfit", "objfun", "K_hat")
  nc <- length(h_names)
  err_o <- matrix(0, n_param, nc)
  err_e <- matrix(0, n_param, nc)
  colnames(err_o) <- h_names
  colnames(err_e) <- h_names
  
  n_o <- nrow(Xo)
  n_e <- nrow(Xe)
  # obj_oe: Xo is used to estimate, Xe is to test
  # obj_eo: Xe is used to estimate, Xo is to test.
  Ko_max <- 0
  Ke_max <- 0
  # i=1; # alph = 1.1; c0 = 0.3
  for (i in 1:n_param) {
    alph <- eg[i, 1]
    c0 <- eg[i, 2]
    pen <- calculate_penalty(X, normalized, penaltyform, alph, c0)
    # pen = p + c0 * (log(n))^(1 + alph)  # same as this one.
    ### for the data with odd index.
    ## op_list_o <- cp_alg(Xo, W, pen = pen)
    op_list_o <- cp_alg(Xo, W, pen = pen)
    ### for X data itself, when calculating cost_matrix, normalized nor not.
    ### For n=p=200, if normalized = FALSE, the algorithm does not work.
    ### For n=200, p=4, whatever normalized is, the algorithm works well.
    tau_hat_o <- op_list_o$cptau
    Lo <- length(tau_hat_o)
    cp_o <- c(0, tau_hat_o, n_o)
    obj_o <- cv_objfun(Xo, Xo, W, cp_o)
    if (obj_o < 1e-4) {obj_o <- 0;}  # IMPORTANT!!!
    obj_oe <- cv_objfun(Xo, Xe, W, cp_o)
    if (obj_oe < 1e-4) {obj_oe <- 0;}  # IMPORTANT!!!
    err_o[i, h_names] <- c(alph, c0, pen, obj_o, obj_oe, Lo)
    # cv_objfun(Xo, Xe, W, cp_o)
    n_diff <- max(Lo - Ko_max, 0)
    if (n_diff > 0) {
      err_o <- cbind(err_o, matrix(0, n_param, n_diff))
      Ko_max <- Ko_max + n_diff
    }
    if (Lo > 0) {err_o[i, (nc+1):(nc + Lo)] <- tau_hat_o}
    
    ### for the data with even index
    op_list_e <- cp_alg(Xe, W, pen = pen)
    tau_hat_e <- op_list_e$cptau
    Le <- length(tau_hat_e)
    cp_e <- c(0, op_list_e$cptau, n_e)
    obj_e <- cv_objfun(Xe, Xe, W, cp_e)
    if (obj_e < 1e-4) {obj_e <- 0;}  # IMPORTANT!!!
    obj_eo <- cv_objfun(Xe, Xo, W, cp_e)
    if (obj_eo < 1e-4) {obj_eo <- 0;}  # IMPORTANT!!!
    err_e[i, h_names] <- c(alph, c0, pen, obj_e, obj_eo, Le)
    n_diff <- max(Le - Ke_max, 0)
    if (n_diff > 0) {
      err_e <- cbind(err_e, matrix(0, n_param, n_diff))
      Ke_max <- Ke_max + n_diff
    }
    if (Le > 0) {err_e[i, (nc+1):(nc + Le)] <- tau_hat_e}
    
  }
  # plot(err_e[, "K_hat"], err_e[, "objfun"])
  # plot(err_e[, "K_hat"], err_e[, "objfit"])
  # plot(err_o[, "K_hat"], err_o[, "objfun"])
  # plot(err_o[, "K_hat"], err_o[, "objfit"])
  res <- list(mat_e = err_e, mat_o = err_o)
  return(res)
}


#' COPSS method with DP algorithm
#' 
#' COPSS method with DP algorithm to find the optimal number of change-points 
#' 
#' The change-point detection algorithm should be DP algorithm.
#' 
#' @param X N*p matrix
#' @param W if normalized is TRUE, then W is inverse of estimated_sigma_square(X); 
#'   if is FALSE, then W = rep(1, ncol(X)). 
#' @param Kmax an upper bound on the true number of change-points
#' @return A list of two matrices mat_e and mat_o. 
#'   mat_o: which uses 'odd' data to estimate, CV error matrix on 'even' data.
#'   mat_e: which uses 'even' data to estimate, CV error matrix on 'odd' data.
copss_dp <- function(X, W, Kmax = 15) {
  # W if normalized is TRUE, then W = estimated_sigma_square(X); 
  # if is FALSE, then W = rep(1, ncol(X)). 
  # obj_o, obj_e are training errors; obj_eo, obj_oe, are testing errors;
  # Xo, Xe are Xodd, Xeven repectively;
  # err_e matrix, where Xo is used to estimate, Xe is to test;
  # err_o matrix, where Xe is used to estimate, Xo is to test.
  Xo <- X[seq(1, nrow(X), 2), ]  # max(seq(1, nrow(X), 2))
  Xe <- X[seq(2, nrow(X), 2), ]  # max(seq(2, nrow(X), 2))
  if (!is.matrix(Xo)) {Xo = matrix(Xo, length(Xo), 1)}
  if (!is.matrix(Xe)) {Xe = matrix(Xe, length(Xe), 1)}
  
  Kmax <- min(Kmax, nrow(X) / 3)   ### tau_mat is Kmax*Kmax matrix
  tau_err_o <- cp_hdmean_dp(Xo, W, Kmax = Kmax)
  tau_err_e <- cp_hdmean_dp(Xe, W, Kmax = Kmax)
  ## for X data itself, when calculating cost_matrix, normalized nor not.
  h_names <- c("objfit", "objfun", "K_hat")
  t_names <- c()
  for (k in 1:Kmax) {t_names <- c(t_names, paste0("tau_", k))}
  err_o <- matrix(0, Kmax, length(h_names) + Kmax)
  err_e <- matrix(0, Kmax, length(h_names) + Kmax)
  colnames(err_o) <- c(h_names, t_names)
  colnames(err_e) <- c(h_names, t_names)
  n_o <- nrow(Xo)
  n_e <- nrow(Xe)
  for (i in 1:Kmax) {  # i =1
    cp_o <- c(0, tau_err_o[i, 1:i], n_o)
    obj_o <- cv_objfun(Xo, Xo, W, cp_o)
    if (obj_o < 1e-4) {obj_o <- 0;}  # IMPORTANT!!!
    obj_oe <- cv_objfun(Xo, Xe, W, cp_o)
    if (obj_oe < 1e-4) {obj_oe <- 0;}  # IMPORTANT!!!
    err_o[i, h_names] <- c(obj_o, obj_oe, i)
    err_o[i, t_names[1:i]] <- tau_err_o[i, 1:i]
    ### The Xe data
    cp_e <- c(0, tau_err_e[i, 1:i], n_e)
    obj_e <- cv_objfun(Xe, Xe, W, cp_e)
    if (obj_e < 1e-4) {obj_e <- 0;}  # IMPORTANT!!!
    obj_eo <- cv_objfun(Xe, Xo, W, cp_e)
    if (obj_eo < 1e-4) {obj_eo <- 0;}  # IMPORTANT!!!
    err_e[i, h_names] <- c(obj_e, obj_eo, i)
    err_e[i, t_names[1:i]] <- tau_err_e[i, 1:i]
  }
  # plot(err_e[, "K_hat"], err_e[, "objfun"])
  # plot(err_e[, "K_hat"], err_e[, "objfit"])
  # plot(err_o[, "K_hat"], err_o[, "objfun"])
  # plot(err_o[, "K_hat"], err_o[, "objfit"])
  res <- list(mat_e = err_e, mat_o = err_o)
  return(res)
}


#' High-dimensional cross-validation (hCV) method with DP algorithm
#' 
#' The hCV method with DP algorithm to find the optimal number of change-points
#' 
#' @inheritParams copss_dp
#' @return A list of two matrices mat_e and mat_o. 
#'   mat_o: which uses 'odd' data to estimate, CV error matrix on 'even' data.
#'   mat_e: which uses 'even' data to estimate, CV error matrix on 'odd' data.
hCV_dp <- function(X, W, Kmax = 15){
  res <- copss_dp(X, W, Kmax = Kmax)
  mat_e <- res$mat_e
  mat_o <- res$mat_o
  p <- ncol(X)
  ## 1. if cv_objfun is sum, then mat_e[, "objfun"] is sum, we need
  ## mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p
  ## 2. if cv_objfun is mean, then mat_e[, "objfun"] is mean, we need
  ## mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p / n
  mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p
  mat_o[, "objfun"] = mat_o[, "objfun"] - mat_e[, "K_hat"] * p
  res <- list(mat_e = mat_e, mat_o = mat_o)
  return(res)
}



#' High-dimensional cross-validation (hCV) method with OP or PELT algorithm
#' 
#' The hCV method to find the optimal number of change-points
#' 
#' The change-point detection algorithm can be OP or PELT.
#' 
#' @inheritParams copss_oppelt
hCV_oppelt <- function(
    X, W, cp_alg = cp_hdmean_op, penaltyform = "CV", normalized = TRUE) {
  res <- copss_oppelt(X, W, cp_alg = cp_alg, penaltyform = "CV", normalized = TRUE)
  mat_e <- res$mat_e
  mat_o <- res$mat_o
  p <- ncol(X)
  ## 1. if cv_objfun is sum, then mat_e[, "objfun"] is sum, we need
  ## mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p
  ## 2. if cv_objfun is mean, then mat_e[, "objfun"] is mean, we need
  ## mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p / n
  mat_e[, "objfun"] = mat_e[, "objfun"] - mat_e[, "K_hat"] * p
  mat_o[, "objfun"] = mat_o[, "objfun"] - mat_e[, "K_hat"] * p
  res <- list(mat_e = mat_e, mat_o = mat_o)
  return(res)
}


#' High-dimensional BIC (hBIC) method with DP algorithm
#' 
#' The hBIC method with DP algorithm to find the optimal number of change-points
#' 
#' The hBIC criterion is calculated with different values of \eqn{\alpha}'s 
#'   in finding the optimal number of change-points. 
#'   
#' The classic BIC criterion is defined as follows:
#' \deqn{BIC(L) = loss_function + L {\log(n)}^{1 + \alpha}}
#'   where loss_function = cv_objfun(X, X, W, cp_hat), L is the estimated muber 
#'   of change-points.
#'   
#' The hBIC criterion is defined as follows:
#' \deqn{hBIC(L) = loss_function + L (\xi_{n, p} + p)}
#'   Then the penalty in hBIC can be formulated as:
#' \deqn{penalty = p + \xi_{n, p} 
#'   = p + c_0 \times \sqrt{trace_R2} \times \log(n)^{1 + \alpha} } 
#' 
#' @inheritParams copss_dp
#' @param pen The penalty
#' @return The error matrix, with columns of "objfit", "objfun", "K_hat", 
#'   followed with the locations of change-points.
hBIC_dp <- function(X, W, Kmax, pen) {
  ## for X data itself, when calculating cost_matrix, normalized nor not.
  n <- nrow(X)
  Kmax <- min(Kmax, nrow(X) / 3)   ### tauMat is Kmax*Kmax matrix
  tauMat <- cp_hdmean_dp(X, W, Kmax = Kmax)
  
  # "objfun" is the BIC obj_fun. "objfit" is the fitting error on X.
  # t_names <- c()
  # for (k in 1:Kmax) {t_names <- c(t_names, paste0("tau_", k))}
  # mapply(paste, "tau", sep='_', 1:Kmax, SIMPLIFY=TRUE, USE.NAMES = FALSE)
  # mapply(paste0, "tau_", 1:Kmax, SIMPLIFY=TRUE, USE.NAMES = FALSE)
  
  t_names = mapply(paste, "tau", sep='_', 1:Kmax, USE.NAMES = FALSE)
  h_names <- c("objfit", "objfun", "K_hat")
  errMat <- matrix(NA, Kmax, length(h_names) + Kmax)  # fitting error on X.
  colnames(errMat) <- c(h_names, t_names)
  
  #### alph = 0.1; c0 = 2.5
  for (k in 1:Kmax) {  # k = 1
    cp_tau <- c(0, tauMat[k, 1:k], n)
    obj_fit <- cv_objfun(X, X, W, cp_tau)
    obj_bic <- obj_fit + k * pen;  # L = k
    if (obj_fit < 1e-4) {obj_fit <- 0;}  # IMPORTANT!!!
    if (obj_bic < 1e-4) {obj_bic <- 0;}  # IMPORTANT!!!
    errMat[k, h_names] <- c(obj_fit, obj_bic, k)
    errMat[k, t_names[1:k]] <- tauMat[k, 1:k]
  }
  return(errMat)
}



#' Calculate the fit error matrix with DP algorithm
#' 
#' Calculate the fit error matrix with DP algorithm without hausdorff distance
#' 
#' @param X N*p matrix
#' @param W vector with p-length as the global normalizer.
#' @param tauMat candidate tau's matrix estimated by DP based on data Xtrain
#' @return An error matrix, with c("objfit", "K_hat") 
#'   and c("tau_1",..., "tau_Kmax") as columns.
dpFitErrMat <- function(X, W, tauMat){
  Kmax <- nrow(tauMat)
  # mapply(paste, "tau", sep='_', 1:Kmax, SIMPLIFY=TRUE, USE.NAMES = FALSE)
  # mapply(paste0, "tau_", 1:Kmax, SIMPLIFY=TRUE, USE.NAMES = FALSE)
  t_names <- mapply(paste, "tau", sep='_', 1:Kmax, USE.NAMES = FALSE)
  h_names <- c("objfit", "K_hat")
  errMat <- matrix(NA, Kmax, length(h_names) + Kmax)  # fitting error on X.
  colnames(errMat) <- c(h_names, t_names)
  
  n <- nrow(X)
  for (k in 1:Kmax) {  # k =1
    cp_tau <- c(0, tauMat[k, 1:k], n)
    obj_fit <- cv_objfun(X, X, W, cp_tau)
    if (obj_fit < 1e-4) {obj_fit <- 0;}  # IMPORTANT!!!
    errMat[k, h_names] <- c(obj_fit, k)
    errMat[k, t_names[1:k]] <- tauMat[k, 1:k]
  }
  # errMat <- errMat[order(errMat[, "K_hat"]), ]
  # diff(errMat[,"objfit"]) > 0
  # plot(errMat[, "K_hat"], errMat[, "objfit"])
  # return(list(mat_bic=errMat))
  return(errMat)
}



#' High-dimensional BIC (hBIC) method with DP algorithm
#' 
#' The hBIC method with DP algorithm to find the optimal number of change-points
#' 
#' The hBIC criterion is calculated with different values of \eqn{\alpha}'s 
#'   in finding the optimal number of change-points. 
#'   
#' The classic BIC criterion is defined as follows:
#' \deqn{BIC(L) = loss_function + L {\log(n)}^{1 + \alpha}}
#'   where loss_function = cv_objfun(X, X, W, cp_hat), L is the estimated muber 
#'   of change-points.
#'   
#' The hBIC criterion is defined as follows:
#' \deqn{hBIC(L) = loss_function + L (\xi_{n, p} + p)}
#'   Then the penalty in hBIC can be formulated as:
#' \deqn{penalty = p + \xi_{n, p} 
#'   = p + c_0 \times \sqrt{trace_R2} \times \log(n)^{1 + \alpha} } 
#' 
#' @inheritParams hBIC_dp
#' @inheritParams copss_dp
#' @param alph_list a list of \eqn{\alpha} values, the \eqn{\alpha} is used
#'   in calculate_penalty. e.g. we can take c(0, 0.1, 0.3, 0.5)
#' @param c0 the constant in calculating the \eqn{\xi_{n, p}}.
#'   The best is c0 = 2.5, whatever \eqn{\alpha} is, the penalty will not be 
#'   too small. c0 = 2.5 is ambitious, the hBIC works well and \eqn{\alpha} can 
#'   be any value. c0 = 2.0 is moderate, the hBIC works for larger \eqn{\alpha},
#'   and not so well for smaller \eqn{\alpha}. 
#'   c0 = 1.5 is conservative, the hBIC may work for larger \eqn{\alpha}.
#' @return An error matrix, with c("hBIC_0", "hBIC_0.1",..., "objfit", "K_hat") 
#'   and c("tau_1",  ..., "tau_Kmax") as columns.
hBIC_dp_bundle <- function(X, W, Kmax, c0, alph_list) {
  ## for X data itself, when calculating cost_matrix, normalized nor not.
  ## The best is c0 = 2.5, whatever alph is, the penalty is not too small
  Kmax <- min(Kmax, nrow(X) / 3)   ### tauMat is Kmax*Kmax matrix
  tauMat <- cp_hdmean_dp(X, W, Kmax)
  
  errMat <- dpFitErrMat(X, W, tauMat)
  
  # t_names = paste("tau", 1:6, sep='_')
  bic_names <- mapply(paste, "hBIC", sep='_', alph_list, USE.NAMES = FALSE)
  bicMat <- matrix(NA, Kmax, length(bic_names))  # fitting error on X.
  colnames(bicMat) <- bic_names
  #### alph = 0.1; c0 = 2.5
  # calculate_penalty(X, normalized, penaltyform = "hBIC", alph, c0)
  # pen <- calculate_penalty(X, normalized, "hBIC", alph = alph, c0 = 2.5)
  
  n <- dim(X)[1]
  p <- dim(X)[2]
  tr2 <- trace_R_square(X)
  for (alph in alph_list) {
    pen <- p + c0 * tr2^(0.5) * log(n)^(1+alph)
    temp <- errMat[, "objfit"] + errMat[, "K_hat"] * pen
    bicMat[, paste("hBIC", alph, sep='_')] <- temp
  }
  
  bicMat <- cbind(bicMat, errMat)
  return(bicMat)
}



#' Calculate the cv error matrix
#' 
#' Calculate the cv error matrix from the tauMat
#' 
#' @param Xtrain train data used to estimate the change-points
#' @param Xtest test data used to calculate the cv error
#' @param W vector with p-length as the global normalizer.
#' @param tauMat candidate tau's matrix estimated by DP based on data Xtrain
#' @return errMat with columns of 
#'   c("objfit", "objfun", "K_hat", "K_diff", "OE", "UE"),
#'   then followed with columns of \eqn{\tau_{1}} to \eqn{\tau_{Kmax}}
dpCVErrMat <- function(Xtrain, Xtest, W, tauMat) {
  # tauMat = tauMat_e; B=Be; Xtrain=Xo; Xtest=Xe
  ## W: vector with p-length as the global normalizer.
  ## tauMat, B, are based on the Xtrain data
  Kmax <- nrow(tauMat)
  # t_names <- c()
  # for (k in 1:Kmax) {t_names <- c(t_names, paste0("tau_", k))}
  t_names <- paste("tau", c(1:Kmax), sep = "_")
  h_names <- c("objfit", "objfun", "K_hat")
  errMat <- matrix(NA, Kmax, length(h_names) + Kmax)
  colnames(errMat) <- c(h_names, t_names)
  n <- nrow(Xtrain)
  for (k in 1:Kmax) {  # k = 1
    ### results fitting on Xtrain, k is the Khat
    cp_hat <- c(0, tauMat[k, 1:k], n)
    obj_fit <- cv_objfun(Xtrain, Xtrain, W, cp_hat)  # fit on Xtrain
    obj_cv <- cv_objfun(Xtrain, Xtest, W, cp_hat)  # fit on Xtrain, test on Xtest
    if (obj_fit < 1e-4) {obj_fit <- 0;}  # IMPORTANT!!!
    if (obj_cv < 1e-4) {obj_cv <- 0;}  # IMPORTANT!!!
    errMat[k, h_names] <- c(obj_fit, obj_cv, k)
    errMat[k, t_names[1:k]] <- tauMat[k, 1:k]
  }
  return(errMat)
}


#' COPSS method with DP algorithm
#' 
#' COPSS method with DP algorithm to find the optimal number of change-points 
#' 
#' copss_dp_ErrMat is the same as copss_dp.
#' 
#' @inheritParams copss_dp
#' @param X N*p matrix
#' W if normalized is TRUE, then W is inverse of estimated_sigma_square(X); 
#'   if is FALSE, then W = rep(1, ncol(X)). 
#' @param Kmax an upper bound on the true number of change-points
#' @return A list of two matrices mat_e and mat_o. The sum of CV errors.
#'   mat_o: which uses 'odd' data to estimate, CV error matrix on 'even' data.
#'   mat_e: which uses 'even' data to estimate, CV error matrix on 'odd' data.
copss_dp_ErrMat <- function(X, W, Kmax = 15) {
  # obj_o, obj_e are training errors; obj_eo, obj_oe, are testing errors;
  # Xo, Xe are Xodd, Xeven repectively;
  # err_e matrix, where Xo is used to estimate, Xe is to test;
  # err_o matrix, where Xe is used to estimate, Xo is to test.
  Xo <- X[seq(1, nrow(X), 2), ]  # max(seq(1, nrow(X), 2))
  Xe <- X[seq(2, nrow(X), 2), ]  # max(seq(2, nrow(X), 2))
  if (!is.matrix(Xo)) {Xo = matrix(Xo, length(Xo), 1)}
  if (!is.matrix(Xe)) {Xe = matrix(Xe, length(Xe), 1)}
  
  Kmax <- min(Kmax, nrow(X) / 3)   ### tau_mat is Kmax*Kmax matrix
  tauMat_o <- cp_hdmean_dp(Xo, W, Kmax = Kmax)
  tauMat_e <- cp_hdmean_dp(Xe, W, Kmax = Kmax)
  err_o <- dpCVErrMat(Xo, Xe, W, tauMat_o)
  err_e <- dpCVErrMat(Xe, Xo, W, tauMat_e)
  # plot(err_e[, "K_hat"], err_e[, "objfun"])
  # plot(err_e[, "K_hat"], err_e[, "objfit"])
  # plot(err_o[, "K_hat"], err_o[, "objfun"])
  # plot(err_o[, "K_hat"], err_o[, "objfit"])
  res <- list(mat_e = err_e, mat_o = err_o)
  return(res)
}



  

